# 졸업작품 일기

더 완성도 있고 이거 가지고 공모전 돌려막기까지 하기 위한 좋은 작품을 만들기 위해 GPT의 사용은 적당히 하고, 제대로 이해하는 것부터 시작해보기로 했다.

### 졸업작품명 : 기타 초보자를 위한 AI 기반 기타 독학 도우미

#### 간단 설명 : 사용자가 기타코드를 잡으면 잡은 코드가 어떤 코드인지, 그 코드를 제대로 잡고 있는지 판별하여 알려주며, 즉각적인 피드백을 통해 학원에 가지 않고도 낮은 가격으로 내 손동작이 정확한지 아닌지 판별할 수 있다.


중간발표 전까지 GPT를 통해 이런저런 시도를 했지만, 이해가 부족한 채로 발표 준비만 열심히 한 느낌이라 제대로 공부하면서, 코드를 디벨롭하기 위해 빡공 시작

![기타코드](img/기타코드_손동작.png)

일단 위의 사진처럼 세밀한 손동작의 차이를 요구하기 때문에 Mediapipe 라이브러리를 사용해야만 한다

# Mediapipe란?
구글에서 제공하는 AU 프레임워크로 인체를 대상으로 하는 인식에 대해 효과적이다 
이미 학습이 완료된 상태로 제공되기 때문에, 파이썬에서 설치 이후 호출만하면 손쉽게 사용 가능

![미디어파이프 예시](img/미디어_파이프_사용예시.png)

손동작 말고도 위 사진처럼 다양한 인체를 인식하고 분류할 수 있다. 우리는 기타코드 인식을 할 것이기 때문에 손동작 인식을 사용하도록 하겠다.


![랜드마크](img/랜드마크.png)

손바닥을 감지한 뒤에 핸드 랜드마크 모델을 통해 위 사진과 같이 손 영역 내에서 21개의 3D 손 관절 좌표에 대한 정확한 키포인트 위치 파악을 수행한다.

인공지능에 관심 있다면 들어봤을 법한 라이브러리이고, 기타 코드 인식에 가장 적합한 친구이지 않을까 싶어 mediapipe를 적극 활용하여 졸업작품을 완성해보고자 한다.


## 1단계 - 데이터셋 수집하기

일단 기타 코드는 정말 깊게 파면 수백 개가 있지만, 기본 메이저 코드 7개라도 제대로 인식하고 디벨롭 시킬 예정 따라서 A B C D E F G 만 먼저 학습하고 테스트한다.

To. GPT

"파이토치 기반으로 할 거고, 아이디어 명은 기타 독학자들을 위한 AI 기반 기타 독학 도우미야.
데이터셋 수집, 훈련, 테스트 py 파일을 3개로 따로 나눌 건데, 먼저 데이터셋 수집을 위한 py 파일만 제공해줘.
데이터셋 수집은 코드별로 일단은 A~G 까지만 진행할 거야. 예를 들어, A코드에 대한 데이터셋을 수집한다고 하면 A 입력 시 캠이 실행되고, 
자동으로 1,000장 정도 빠른 속도로 내 손을 인식하여 캡쳐한 뒤, A 파일에 저장해줘"

파이토치를 쓰는 이유는 이상하게도 내 컴퓨터에 죽어도 텐서플로우랑 gpu 호환이 안돼서 그렇다

그렇게 나온 코드 ↓

```python

import cv2
import os
import mediapipe as mp
import json

# MediaPipe 초기화
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

# 데이터셋 저장 경로
DATASET_DIR = "./guitar_dataset"
CODES = ['A', 'B', 'C', 'D', 'E', 'F', 'G']

def create_folders():
    """
    A~G 폴더를 생성합니다.
    """
    if not os.path.exists(DATASET_DIR):
        os.makedirs(DATASET_DIR)
    for code in CODES:
        code_dir = os.path.join(DATASET_DIR, code)
        if not os.path.exists(code_dir):
            os.makedirs(code_dir)

def capture_hand_data(target_code, num_images=1000, delay=0.1):
    """
    손 데이터 및 이미지를 캡처합니다.
    :param target_code: A~G 중 캡처할 코드
    :param num_images: 캡처할 데이터 수
    :param delay: 각 캡처 사이의 지연 시간 (초)
    """
    if target_code not in CODES:
        print(f"잘못된 코드 입력: {target_code}. A~G 중에서 선택하세요.")
        return

    code_dir = os.path.join(DATASET_DIR, target_code)
    landmarks_file = os.path.join(code_dir, f"{target_code}_landmarks.json")
    landmarks_data = []  # 랜드마크 데이터 저장용

    cap = cv2.VideoCapture(0)  # 웹캠 열기

    if not cap.isOpened():
        print("웹캠을 열 수 없습니다.")
        return

    with mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5) as hands:
        print(f"{target_code} 코드의 데이터를 캡처 중입니다. ESC 키를 누르면 중단됩니다.")
        count = 0

        while count < num_images:
            ret, frame = cap.read()
            if not ret:
                print("캡처 실패. 다시 시도 중...")
                continue

            # 이미지를 RGB로 변환 (MediaPipe는 RGB 이미지를 사용)
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = hands.process(rgb_frame)

            if results.multi_hand_landmarks:
                # 첫 번째 손만 추적
                hand_landmarks = results.multi_hand_landmarks[0]

                # 랜드마크 데이터를 추출하여 리스트로 변환
                landmarks = [{
                    'x': lm.x,
                    'y': lm.y,
                    'z': lm.z
                } for lm in hand_landmarks.landmark]

                # 랜드마크 데이터 저장
                landmarks_data.append({
                    'image_id': f"{target_code}_{count:04d}.jpg",
                    'landmarks': landmarks
                })

                # 이미지를 저장
                img_path = os.path.join(code_dir, f"{target_code}_{count:04d}.jpg")
                cv2.imwrite(img_path, frame)
                count += 1
                print(f"{count}/{num_images} 이미지 저장 및 랜드마크 추출 완료: {img_path}")

                # 랜드마크를 이미지에 그리기 (디버깅용)
                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

            # 실시간 화면 출력
            cv2.imshow("Capturing", frame)

            # ESC 키로 중단
            if cv2.waitKey(int(delay * 1000)) & 0xFF == 27:
                print("캡처 중단")
                break

    # JSON 파일로 랜드마크 데이터 저장
    with open(landmarks_file, 'w') as f:
        json.dump(landmarks_data, f, indent=4)

    cap.release()
    cv2.destroyAllWindows()
    print(f"{target_code} 데이터 캡처 완료. 총 {count}장 저장 및 랜드마크 데이터 저장 완료.")

if __name__ == "__main__":
    create_folders()
    print("A~G 코드 중 하나를 선택하세요.")
    target_code = input("캡처할 코드 입력 (A~G): ").strip().upper()
    capture_hand_data(target_code)

```


호환성 에러는 라이브러리들을 여러 번 삭제하고 추가하고 하다보니 해결되었다.

이제 코드를 하나씩 분석해보고 문제 없는지 파악해보자.
그냥 단 한 라인도 안 놓치고 분석할거임

```python
import cv2               # 이미지 및 비디오 작업을 위한 (캠 킬 때)
import os                # 파일과 디렉토리를 다루기 위한 (코드 저장할 폴더 생성할 때)
import mediapipe as mp   # 손 추적, 랜드마크 제공 라이브러리
import json              # 데이터를 json 형식으로 저장하기 위함 -> 이미지가 아니라 손의 랜드마크 좌표를 저장하는 것. 
                         # 이로 인해 관절 좌표를 바로 학습 데이터로 사용하면 모델이 손 모양과 위치를 더 빠르고 효과적으로 학습할 수 있다고 한다.
                         # z좌표를 포함시킬 수 있다. 손이 카메라에서 얼마나 가까운지를 알 수 있는데, 이미지에서는 이 정보를 포함되지 않는다.

# MediaPipe 초기화
mp_hands = mp.solutions.hands                  # 손 랜드마크 추적 모듈 
mp_drawing = mp.solutions.drawing_utils        # 랜드마크와 손가락의 연결선을 이미지에 시각적으로 그릴 수 있는 기능 제공

# 데이터셋 저장 경로
DATASET_DIR = "./guitar_dataset"               # 데이터셋을 저장할 디렉토리 경로
CODES = ['A', 'B', 'C', 'D', 'E', 'F', 'G']    # 기타 코드 리스트
```

원래 형준이랑 시도할 때 이미지 파일로 저장하고 실시간 테스트가 잘 안됐었는데, 좌표로 저장이라니 벌써 성공각이 보인다;; 설렘



```python
def create_folders():
    """
    A~G 폴더를 생성합니다.
    """
    if not os.path.exists(DATASET_DIR):            # 데이터셋 디렉토리가 없다면
        os.makedirs(DATASET_DIR)                   # 새로 만들기
    for code in CODES:
        code_dir = os.path.join(DATASET_DIR, code) # A~G 코드를 guitar_dataset 폴더 안에 생성. 뭐 이런 건 주석 안 달아도 되겠다
        if not os.path.exists(code_dir):
            os.makedirs(code_dir)
```

지금까지 데이터 저장할 폴더를 만들었다.

```python
def capture_hand_data(target_code, num_images=1000, delay=0.1):              # 1000장 캡쳐, 캡쳐 사이 지연 시간 0.1초. 아무래도 1초에 10장을 찍는 듯
    """                                                                      
    손 데이터 및 이미지를 캡처합니다.
    :param target_code: A~G 중 캡처할 코드
    :param num_images: 캡처할 데이터 수
    :param delay: 각 캡처 사이의 지연 시간 (초)
    """
    if target_code not in CODES:                                              # 100초 정도는 코드 잡고 캠 앞에 있어줄만한 시간인 듯 하다
        print(f"잘못된 코드 입력: {target_code}. A~G 중에서 선택하세요.")        # 입력이 A~G 범위를 벗어나면 에러 출력
        return

    code_dir = os.path.join(DATASET_DIR, target_code)                         # 각 코드별 경로 지정
    landmarks_file = os.path.join(code_dir, f"{target_code}_landmarks.json")  # 손 랜드마크 데이터를 json 파일로 저장할 경로 설정 
    landmarks_data = []                                                       # 랜드마크 데이터 저장할 리스트 초기화       

    cap = cv2.VideoCapture(0)            # 웹캠 열기

    if not cap.isOpened():
        print("웹캠을 열 수 없습니다.")
        return

    with mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5) as hands:  # False=실시간 영상을 처리하겠다, 한손만 인식, 0.5이상이면 손으로 인식
        print(f"{target_code} 코드의 데이터를 캡처 중입니다. ESC 키를 누르면 중단됩니다.")
        count = 0

        while count < num_images:                   # 카메라에서 프레임 읽기, 건너 뛸만 한 듯
            ret, frame = cap.read()                 # ret -> 성공 여부,  fram -> 현재 프레임 데이터(이미지)
            if not ret:
                print("캡처 실패. 다시 시도 중...")
                continue

            # 이미지를 RGB로 변환 (MediaPipe는 RGB 이미지를 사용하므로 변환이 필요합니다)
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # 왜냐면 OpenCV는 기본적으로 이미지를 BGR 형식으로 제공한다.
            results = hands.process(rgb_frame)                 # 랜드마크 추적 기능을 호출

            if results.multi_hand_landmarks:
                # 첫 번째 손만 추적
                hand_landmarks = results.multi_hand_landmarks[0]

                # 랜드마크 데이터를 추출하여 리스트로 변환
                landmarks = [{
                    'x': lm.x,
                    'y': lm.y,
                    'z': lm.z
                } for lm in hand_landmarks.landmark]

                # 랜드마크 데이터 저장
                landmarks_data.append({
                    'image_id': f"{target_code}_{count:04d}.jpg",
                    'landmarks': landmarks
                })

                # 이미지를 저장
                img_path = os.path.join(code_dir, f"{target_code}_{count:04d}.jpg")
                cv2.imwrite(img_path, frame)
                count += 1
                print(f"{count}/{num_images} 이미지 저장 및 랜드마크 추출 완료: {img_path}")

                # 랜드마크를 이미지에 그리기 (디버깅용)
                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

            # 실시간 화면 출력
            cv2.imshow("Capturing", frame)

            # ESC 키로 중단
            if cv2.waitKey(int(delay * 1000)) & 0xFF == 27:
                print("캡처 중단")
                break

    # JSON 파일로 랜드마크 데이터 저장
    with open(landmarks_file, 'w') as f:
        json.dump(landmarks_data, f, indent=4)

    cap.release()
    cv2.destroyAllWindows()
    print(f"{target_code} 데이터 캡처 완료. 총 {count}장 저장 및 랜드마크 데이터 저장 완료.")
```


```python
if __name__ == "__main__":
    create_folders()
    print("A~G 코드 중 하나를 선택하세요.")
    target_code = input("캡처할 코드 입력 (A~G): ").strip().upper()
    capture_hand_data(target_code)
```


문제없이 잘 실행되는 코드인 것을 확인한 뒤 코드별로 1000장씩 데이터셋을 쌓았다(너무 느려서 delay=0.05로 변경하였음. 그래도 오래 걸리더라)

![데이터셋수집](img/F_0011.jpg)

이런 식으로 데이터를 쌓고

![json형태](img/랜드마크좌표.png)

이런 식으로 랜드마크가 저장 된다. 이제 데이터를 다 쌓았으니 A~G 에 대한 판별을 잘 할 수 있도록 도와주는 코드를 짜보자

To. GPT

"위 코드를 바탕으로 이제 모델을 훈련시키고자 해. 각 코드에 대해 train 비율을 8로, valid 비율을 2로 해서 학습해주고
전체 데이터에 대해 20퍼센트만 테스트 데이터로서 사용하여 학습시켜줘. 모델을 분류 학습에 뛰어난 사전학습 모델이 존재한다면 그것을 써주면 되고,
그런게 없다면 적절히 좋은 모델을 설계해줘. 현재의 코드에서는 '학습'만 진행할 거고 추후에 다른 파일에서 실시간 테스트를 진행할 거야
학습 과정을 눈으로 볼 수 있도록 tqdm 라이브러리를 활용해줘"

tqdm 이 놈은 그래프 형태로 학습 진행과정을 볼 수 있는데, 이거 없으면 잘 되고 있는 건지 렉 걸린건지 헷갈려서 개답답하니 꼭 추가할 것


```python
import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import models, transforms
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import numpy as np
from PIL import Image

# 경로 설정
DATASET_DIR = "./guitar_dataset"
CODES = ['A', 'B', 'C', 'D', 'E', 'F', 'G']
NUM_CLASSES = len(CODES)

# 데이터셋 정의
class GuitarDataset(Dataset):
    def __init__(self, dataset_dir, transform=None):
        self.data = []
        self.labels = []
        self.transform = transform
        self.label_map = {code: idx for idx, code in enumerate(CODES)}

        # JSON 파일에서 데이터를 읽어옴
        for code in CODES:
            code_dir = os.path.join(dataset_dir, code)
            landmarks_file = os.path.join(code_dir, f"{code}_landmarks.json")
            if os.path.exists(landmarks_file):
                with open(landmarks_file, 'r') as f:
                    landmarks = json.load(f)
                    for item in landmarks:
                        img_path = os.path.join(code_dir, item['image_id'])
                        if os.path.exists(img_path):
                            self.data.append((img_path, self.label_map[code]))
                            self.labels.append(self.label_map[code])

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        img_path, label = self.data[idx]
        image = Image.open(img_path).convert("RGB")
        if self.transform:
            image = self.transform(image)
        return image, label

# 데이터셋 준비
def prepare_datasets(dataset_dir, test_size=0.2, train_valid_split=0.8):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    full_dataset = GuitarDataset(dataset_dir, transform=transform)

    # 전체 데이터를 train+valid와 test로 분리
    train_valid_size = int(len(full_dataset) * (1 - test_size))
    test_size = len(full_dataset) - train_valid_size
    train_valid_dataset, test_dataset = random_split(full_dataset, [train_valid_size, test_size])

    # train+valid 데이터를 train과 valid로 분리
    train_size = int(len(train_valid_dataset) * train_valid_split)
    valid_size = len(train_valid_dataset) - train_size
    train_dataset, valid_dataset = random_split(train_valid_dataset, [train_size, valid_size])

    return train_dataset, valid_dataset, test_dataset

# 모델 정의
def build_model(num_classes):
    model = models.resnet18(pretrained=True)  # ResNet18 사전학습 모델
    num_features = model.fc.in_features
    model.fc = nn.Linear(num_features, num_classes)  # 분류를 위한 Fully Connected Layer
    return model

# 학습 함수
def train_model(model, train_loader, valid_loader, criterion, optimizer, device, epochs=10):
    model = model.to(device)
    best_valid_loss = float('inf')

    for epoch in range(epochs):
        print(f"Epoch {epoch+1}/{epochs}")

        # Training Phase
        model.train()
        train_loss = 0.0
        train_correct = 0

        train_loop = tqdm(train_loader, desc="Training")
        for images, labels in train_loop:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * images.size(0)
            _, preds = torch.max(outputs, 1)
            train_correct += torch.sum(preds == labels.data)
            train_loop.set_postfix(loss=loss.item())

        train_loss /= len(train_loader.dataset)
        train_accuracy = train_correct.double() / len(train_loader.dataset)

        # Validation Phase
        model.eval()
        valid_loss = 0.0
        valid_correct = 0

        valid_loop = tqdm(valid_loader, desc="Validation")
        with torch.no_grad():
            for images, labels in valid_loop:
                images, labels = images.to(device), labels.to(device)

                outputs = model(images)
                loss = criterion(outputs, labels)

                valid_loss += loss.item() * images.size(0)
                _, preds = torch.max(outputs, 1)
                valid_correct += torch.sum(preds == labels.data)
                valid_loop.set_postfix(loss=loss.item())

        valid_loss /= len(valid_loader.dataset)
        valid_accuracy = valid_correct.double() / len(valid_loader.dataset)

        print(f"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}")
        print(f"Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}")

        # Save the best model
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            torch.save(model.state_dict(), "best_model.pth")
            print("Best model saved.")

# 학습 데이터 준비
train_dataset, valid_dataset, test_dataset = prepare_datasets(DATASET_DIR, test_size=0.2, train_valid_split=0.8)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# 모델 생성 및 학습
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

model = build_model(NUM_CLASSES)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.fc.parameters(), lr=0.001)

train_model(model, train_loader, valid_loader, criterion, optimizer, device, epochs=10)

# 테스트 성능 평가 (선택 사항)
model.load_state_dict(torch.load("best_model.pth"))
model.eval()

test_correct = 0
with torch.no_grad():
    test_loop = tqdm(test_loader, desc="Testing")
    for images, labels in test_loop:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, preds = torch.max(outputs, 1)
        test_correct += torch.sum(preds == labels.data)

test_accuracy = test_correct.double() / len(test_loader.dataset)
print(f"Test Accuracy: {test_accuracy:.4f}")
```

위 프롬프트 요청사항에 따라 RestNet18 모델을 사용한 것을 볼 수 있다.
코드 분석 전에 ResNet18이 어떤 놈인지부터 확실하게 파악하고 넘어가야겠다.


## ResNet18

ImageNet 데이터셋으로 학습된 모델로, 18개 층으로 이루어진 ResNet을 의미
VGGNet이 밝힌 Layer가 깊어질수록 성능이 좋아진다는 사실을 심층적으로 연구한 결과
Layer가 너무 깊어져도 성능이 떨어지는 현상을 발견ㄷㄷ 왜 와이 -> Layer가 깊어질수록 미분을 점점 많이하게 되는데
미분값이 점점 작아져서 가중치의 영향이 작아지는 Gradient Vanishing(기울기 소실)이 발생. -> Training Data로 학습이 되지 않는 문제가 발생

따라서 ResNet은 skip connection을 이용한 residual learning을 통해 gradient vanishing 문제를 해결 (????????????)

--------------------------다시 쉬운 설명---------------------------

딥러닝 모델이 역전파를 통해 가중치를 업데이트 할 때, 각 계층의 미분값이 점점 작아질테니 레이어가 깊어질수록(입력쪽으로 갈수록)
미분값이 0에 수렴하게 되면서 입력에 가까운 초반 레이어는 학습이 거의 이루어지지 않는 문제다. 

skip connection = 네트워크 일부 계층을 건너 뛰기 -> 기울기 소실 문제 완화
residual learning -> 출력 y를 직접 학습하는 대신, 차이(residual)인 y-x 를 학습 -> 학습이 더 간단하고 안정적으로 이루어지는 데에 도움
-> 이게 왜 더 간단하고 안정적인가? -> 실제로 y와 x가 큰 차이가 없다면, R(x)는 0에 가까운 간단한 값을 학습하면 되기 때문임 
so, y=F(x) -> R(x)=y-x -> y=R(x)+x -> F(x)=R(x)+x

#### Shortcut에 대하여
일반적으로 딥러닝의 각 레이어는 입력 값을 처리해서 새로운 출력을 만듦 예를 들어: y=F(x)
하지만 Shortcut에서는 이렇게 처리한 출력에 입력 x를 그냥 더해줌 y=F(x)+x
이러면 x의 원래 정보를 보존하면서 추가적으로 F(x)의 결과를 더할 수 있다. 
결과적으로 레이어가 깊어져도 Gradient가 사라지지 않고 안정적으로 학습된다.
이 기능은 활성화 함수 이전에 추가됨.(활성화 함수 이후에 더하면 비선형 효과로 입출력이 단순히 합쳐지지 않음)

활성화 함수 이전 : y = F(x) + x 
활성화 함수 이후 : ReLU(F(x)) + x  뭔말알? ㅇㅇ 

결론적으로 ResNet은 이러한 설계로 인해 깊은 네트워크에서도 성능 저하 없이 학습이 가능하다.

ResNet은 18 이외에도 34, 50, 101, 152 등이 있는데 뒤의 숫자는 레이어의 깊이를 의미하며 레이어가 많아질수록 더 복잡한 패턴을 학습할 수 있다.
데이터셋이 작거나 단순한 경우, 너무 깊은 네트워크는 과적합이 발생할 수 있는데, 아무래도 기타 코드 인식은 데이터도 많고 복잡한 편에 속하니
ResNet50을 먼저 사용해보고 나오는 결과 값에 따라 모델을 수정해봐야겠다.


그래서 변경된 학습 코드

```python
import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import models, transforms
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import numpy as np
from PIL import Image

# 경로 설정
DATASET_DIR = "./guitar_dataset"
CODES = ['A', 'B', 'C', 'D', 'E', 'F', 'G']
NUM_CLASSES = len(CODES)

# 데이터셋 정의
class GuitarDataset(Dataset):
    def __init__(self, dataset_dir, transform=None):
        self.data = []
        self.labels = []
        self.transform = transform
        self.label_map = {code: idx for idx, code in enumerate(CODES)}

        # JSON 파일에서 데이터를 읽어옴
        for code in CODES:
            code_dir = os.path.join(dataset_dir, code)
            landmarks_file = os.path.join(code_dir, f"{code}_landmarks.json")
            if os.path.exists(landmarks_file):
                with open(landmarks_file, 'r') as f:
                    landmarks = json.load(f)
                    for item in landmarks:
                        img_path = os.path.join(code_dir, item['image_id'])
                        if os.path.exists(img_path):
                            self.data.append((img_path, self.label_map[code]))
                            self.labels.append(self.label_map[code])

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        img_path, label = self.data[idx]
        image = Image.open(img_path).convert("RGB")
        if self.transform:
            image = self.transform(image)
        return image, label

# 데이터셋 준비
def prepare_datasets(dataset_dir, test_size=0.2, train_valid_split=0.8):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    full_dataset = GuitarDataset(dataset_dir, transform=transform)

    # 전체 데이터를 train+valid와 test로 분리
    train_valid_size = int(len(full_dataset) * (1 - test_size))
    test_size = len(full_dataset) - train_valid_size
    train_valid_dataset, test_dataset = random_split(full_dataset, [train_valid_size, test_size])

    # train+valid 데이터를 train과 valid로 분리
    train_size = int(len(train_valid_dataset) * train_valid_split)
    valid_size = len(train_valid_dataset) - train_size
    train_dataset, valid_dataset = random_split(train_valid_dataset, [train_size, valid_size])

    return train_dataset, valid_dataset, test_dataset

# 모델 정의
def build_model(num_classes):
    model = models.resnet50(pretrained=True)  # ResNet50 사전학습 모델
    num_features = model.fc.in_features
    model.fc = nn.Linear(num_features, num_classes)  # 분류를 위한 Fully Connected Layer
    return model

# 학습 함수
def train_model(model, train_loader, valid_loader, criterion, optimizer, device, epochs=10):
    model = model.to(device)
    best_valid_loss = float('inf')

    for epoch in range(epochs):
        print(f"Epoch {epoch+1}/{epochs}")

        # Training Phase
        model.train()
        train_loss = 0.0
        train_correct = 0

        train_loop = tqdm(train_loader, desc="Training")
        for images, labels in train_loop:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * images.size(0)
            _, preds = torch.max(outputs, 1)
            train_correct += torch.sum(preds == labels.data)
            train_loop.set_postfix(loss=loss.item())

        train_loss /= len(train_loader.dataset)
        train_accuracy = train_correct.double() / len(train_loader.dataset)

        # Validation Phase
        model.eval()
        valid_loss = 0.0
        valid_correct = 0

        valid_loop = tqdm(valid_loader, desc="Validation")
        with torch.no_grad():
            for images, labels in valid_loop:
                images, labels = images.to(device), labels.to(device)

                outputs = model(images)
                loss = criterion(outputs, labels)

                valid_loss += loss.item() * images.size(0)
                _, preds = torch.max(outputs, 1)
                valid_correct += torch.sum(preds == labels.data)
                valid_loop.set_postfix(loss=loss.item())

        valid_loss /= len(valid_loader.dataset)
        valid_accuracy = valid_correct.double() / len(valid_loader.dataset)

        print(f"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}")
        print(f"Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}")

        # Save the best model
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            torch.save(model.state_dict(), "best_model.pth")
            print("Best model saved.")

# 학습 데이터 준비
train_dataset, valid_dataset, test_dataset = prepare_datasets(DATASET_DIR, test_size=0.2, train_valid_split=0.8)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# 모델 생성 및 학습
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

model = build_model(NUM_CLASSES)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.fc.parameters(), lr=0.001)

train_model(model, train_loader, valid_loader, criterion, optimizer, device, epochs=10)

# 테스트 성능 평가 (선택 사항)
model.load_state_dict(torch.load("best_model.pth"))
model.eval()

test_correct = 0
with torch.no_grad():
    test_loop = tqdm(test_loader, desc="Testing")
    for images, labels in test_loop:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, preds = torch.max(outputs, 1)
        test_correct += torch.sum(preds == labels.data)

test_accuracy = test_correct.double() / len(test_loader.dataset)
print(f"Test Accuracy: {test_accuracy:.4f}")
```



