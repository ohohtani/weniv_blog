# 목적함수

강의 자료 첫 부분에 이런 글이 나온다.

"시험에서는 틀린 만큼 합당한 벌점을 받는 것이 중요하다. 그래야 다음 시험에서 심기일전으로 공부하여 틀리는 개수를 줄일 가능성이 크기 때문이다. 틀린 개수에 상관없이 비슷한 발점을 받는다면 나태해져 성적을 올리는 데 지연이 발생할 것이다. 이러한 원리가 기계 학습에도 적용될까?"

바로 다음 페이지에 평균제곱 오차(MSE)가 나오는데, 이것이 벌점과 같은 작용을 하는 듯 하다.

평균제곱 오차는 다음과 같이 나타낸다.
![alt text](img/딥러닝_최적화/image.png)

y(정답값)와 o(예측값)의 차이가 클 수록 e 값이 커지므로, 벌점의 형태로서 적합하다고 볼 수 있다.

하지만 큰 허점이 존재한다.

![alt text](img/딥러닝_최적화/image1.png)

위 사진의 경우, 왼쪽 그림이 오른쪽 그림보다 정답값-예측값의 크기가 작으므로, 왼쪽그림의 벌점이 더 적게 나와야 정상이다.
하지만 계산을 해보니 오른쪽 신경망의 gradient 값이 더 작다.
gradient 값이 작다는 것은 '모델 파라미터를 수정할 필요가 적다는 것'을 의미한다.
벌점은 더 많이 받으면서 수정할 필요가 적은 모순적인 상황이 발생하기 때문에 이 녀석(MSE)은 목적함수로서 부적절하다고 판단할 수 있다.

## 교차 엔트로피 목적함수

Cross Entropy 라고 하는 놈인데 실제로 여러 학습 코드를 돌리면서 상당히 자주 본 친구다.
이 녀석은 적합한 목적함수임이 틀림 없을 것이다.

그 전에 먼저 엔트로피란 무엇인가?

Entropy : 정보를 표현하는 데 필요한 최소 자원량(bits의 단위로 표현 되는 0 or 1의 길이) 즉, 최소 기댓값

예시로.. 친한 친구와의 카톡 대화에서 'ㅋ'을 상대적으로 많이 치니까 P(ㅋ)=0.5 , P(술)=0.3 , P(공부)=0.01 정도로 잡을 수 있겠다.
그러므로 많이 치는 'ㅋ'을 짧 코딩하고(예: 0으로 표현) '공부'를 길게 코딩해야(예: 111로 표현) 효율적일 것이다.
요약하면, "확률이 작을수록 길이가 길어진다." 
![alt text](img/딥러닝_최적화/image2.png)

설명을 그래프로 표현하면 위 사진과 같고 이는 -log 그래프 형태를 띈다.
bit로 표현해야 되기 때문에 log의 밑은 2가 되고, 확률로써 표현해야 되기 때문에 -log2(Pi)가 되며,
기댓값을 구하기 위해 각 길이(도수)와 확률을 곱한 값을 전부 더해주어야 한다.
길이는 -log2(Pi) 인 것이고, 확률은 Pi 이므로 최종 식은

![alt text](img/딥러닝_최적화/image3.png)


하지만 Cross Entropy 에서는 이 확률들을 모두 균등하게 잡는다. P는 실제확률이고, 내가 생각한 확률은 Q로서 표현한다 (Q(ㅋ)=0.01, Q(술)=0.01, Q(공부)=0.01)
나머지 부분은 Entropy와 동일하고 P대신 Q로 바꿔주면 되기 때문에 Cross Entropy의 최종식은 Entropy 식에서 P만 Q로 바꿔주면 된다.

이를 벌점으로서 유용한지 확인하기 위해 아래 식에 대입해보면

![alt text](img/딥러닝_최적화/image4.png)

정답값과 예측값의 차이가 클 때 더 큰 벌점을 부여하는 것을 확인. 1단계는 통과했고,
그레디언트까지만 통과하면 목적함수로서 적합하다고 할 수 있다.

미분과정을 생략하고 표현하면

![alt text](img/딥러닝_최적화/image5.png)

로 나타내지고, 값을 대입해보면 그레디언트도 벌점에 비례하여 나타나는 것을 확인하였다.

결론 : Cross Entropy는 목적함수로서 적합하다!


## Softmax

Multi Classification 을 위해 사용하는데, 상대 평가가 가능하며 무조건 1보다 작고, 각 확률의 합은 1이다.
식으로 나타내면 아래와 같다.

![alt text](img/딥러닝_최적화/image6.png)

![alt text](img/딥러닝_최적화/image7.png)

Sigmoid와는 달리, softmax는
최대값을 더욱 활성화하고 작은 값을 억제하는 효과가 있다. 이는 목적에 따라 유용하게 쓸 수 있는데, 예를 들어 softmax의 출력에 따라 가장 큰 값과 두 번째 큰 값의 차이가
임곗값보다 작으면 분류를 포기하고 기각하는 전략을 쓸 수 있다.

강의 자료에 그렇게 많은 내용이 있지 않으니, 여기까지 하고 패스



# 성능 향상

## 데이터 전처리

규모 문제에 대해 다뤄보자.

사람 2명에 대한 데이터가 있다고 치자.

키 : 1.855m , 1.525m
몸무게 : 65.5kg, 45.0kg

두 사람에 대한 키 차이는 0.33 정도
두 사람에 대한 몸무게 차이는 20 정도

단위 때문에 두 특징값 차이가 70배 정도 발생한다.
이렇게 되면 첫 번째 특징에 연결된 가중치는 두 번째 특징에 연결된 가중치에 비해 70배 정도 느리게 학습되는 것이다.

또한, 위처럼 모든 특징이 양수인 경우에도 문제가 발생한다.

이는 경사 하강법에서 적용 되는 내용인데, 경사 하강법은 손실 함수를 최솟화하기 위해 사용한다.
여기서는 기울기와 학습률이 영향을 미치는데,
만약 모든 특성이 양수라면 기울기 변화가 너무 급격하게 일어날 수 있어 이를 적절히 조절하지 않으면 학습이 느리게 이루어질 수 있다.


## 표준화

규모 문제와 데이터 양수 문제를 해결해주는 것이 표준화이다.

표준화는 데이터를 평균 0, 분산 1로 변환하는 방식이다.

![alt text](img/딥러닝_최적화/image8.png)

## 정규화

데이터를 0과 1 사이의 범위로 변환하는 방식. 주로 최솟값과 최대값을 사용하여 반환

![alt text](img/딥러닝_최적화/image9.png)

데이터셋이 [1,2,3,4,5] 일 때 대입하면 모두 0과 1 사이(0과 1 포함) 범위로 조정 됨. => 규모 문제 해결

## 원 핫 인코딩
키, 몸무게 같은 데이터와 달리 성별이나, 체질(예: 태양인, 소양인 등..)은 거리 개념(물리적 크기)을 갖지 않는다.
따라서 남자가 여러명일 때 남자 1, 남자 2 이런 식으로 표현하는 게 아니라. 성별의 경우 남 녀 2개 뿐이니까 
남자면 1 0  여자면 0 1 이런 식으로 표한한다. -> 값의 개수만큼 비트를 부여한다. 

이는 수학적으로는 값이 다를 수 있지만, 그 차이가 실제 크기나 거리 개념을 의미하지 않는다라는 의미를 전달한다.

## 가중치 초기화
신경망의 가중치는 난수를 생성하여 초기화해야 한다.

이유 : 신경망에서 모든 가중치를 동일한 값으로 초기화하면, 모든 뉴런들이 동일한 값을 학습하게 되어 '대칭이 깨지지 않는다'
-> 즉, 각 뉴런이 서로 다르게 학습할 기회가 없어진다. 따라서 난수 초기화는 각 뉴런의 가중치를 다르게 설정하여 각각 다른 패턴을 학습할 수 있도록 돕는다.

가중치 초기화 시, 가우시안 정규분포 혹은 균일분포에서 값을 추출하는 데 둘 중 어떤 것을 사용해도 성능 차이는 거의 없다.
중요한 것은 초기화의 범위 설정인데,

![alt text](img/딥러닝_최적화/image10.png)

너무 작은 값으로 초기화 하면, 0에 가까운 값으로 시작하니까 정보가 사라지거나 학습이 잘 안 될수 있다
너무 큰 값으로 초기화 하면, 값이 폭주하거나, 네트워크가 불안정해져서 학습이 어려워진다.

따라서 위 사진은 '최적의 범위를 제공하는 초기화 범위 계산 공식' 정도로 알아두면 되겠다.

bias는 보통 0으로 초기화하며, 대표적으로 AlexNet 과 ResNet 에서 이를 사용하였다. 

## 모멘텀
그레디언트에 '스무딩'을 가하여 잡음 효과를 줄인다 -> 수렴 속도 향상의 효과
![alt text](img/딥러닝_최적화/image13.png)

속도 벡터 v는 이전 그레디언트를 누적한 것에 해당한다.(처음 v는 0)
알파값의 효과는 알파가 0이면 적용이 안 된 것과 같고, 
알파가 1에 가까울수록 이전 그레디언트의 가중치를 많이 사용하는 느낌. 통상적으로 0.5, 0.9, 0.99를 사용한다.


![alt text](img/딥러닝_최적화/image11.png)

검은선이 모멘텀을 적용하지 않은 것이고, 파란선이 모멘텀을 적용한 것이다.
검은선을 보면 이동량이 너무 커서 적절한 곳을 지나치는 'Overshooting'이 발생하지만, 
파란선의 경우 Overshooting 현상이 감소하였고, 검은선에 비해 적은 반복 횟수로 최적해를 찾아간다. 

### 네스테로프 모멘텀
![alt text](img/딥러닝_최적화/image12.png)

현재는 기존 모멘텀 기법을 개선한 '네스테로프 모멘텀'이 널리 사용되고 있다.
현재 v 값으로 다음 이동할 곳을 예견한 후, 예견한 곳의 그레디언트를 사용하는 것.


## 적응적 학습률
![alt text](img/딥러닝_최적화/image14.png)

학습률이 너무크면 검은선 처럼 오벼슈팅 현상이, 
학습률이 너무 작으면 파란선 처럼 수렴이 느린 현상이 발생한다.
단순히 그레디언트에 학습률 p를 곱하면 모든 매개변수가 같은 크기의 학습률을 사용하는 셈이라 최적화가 되지 못한다.
따라서 적응적 학습률은 매개변수마다(각 세타별로) 자신의 상황에 따라 다른 학습률을 조절해서 사용한다; [학습률 답금질]

AdaGrad(단순히 제곱을 더함) -> RMSProp(가중치 이동 평균) -> Adam(RMSProp + Momentum)


## Activation Function
활성값 z를 계산하고 활성함수 τ를 적용하는 과정

![alt text](img/딥러닝_최적화/image15.png)

![alt text](img/딥러닝_최적화/image16.png)

tanh의 경우 활성값이 커지면 포화상태가 되고 그레디언트가 0에 가까워지는데, 
이는 매개변수 갱신이 매우 느린 요인이 된다.

가장 오른쪽에 나와있는 그래프인 ReLU는 이러한 포화문제를 해소한다.
좀 더 쉽게 설명하자면, 면 양의 값에서는 선형이기 때문에 큰 값에서 기울기가 너무 작아져서 학습이 느려지는 문제가 없다.
반면, 음의 값에서는 출력이 0이 되기 때문에 희소한(신경망에서 많은 뉴런 또는 가중치가 0이 되는 상태) 신경망을 만든다.
예를 들어, 신경망이 처음에 무작위로 초기화되면, 대략 절반 정도의 뉴런에서 입력 값이 음수가 될 수 있다.
이 때, 음수인 뉴런들은 모두 출력이 0이 되어 학습에 영향을 미치지 않게 되는데(계산에서 제외됨), 이렇게 되면 신경망이 희소한 상태가 되어,
각 뉴런이 특정 특징을 담당하게 되며, 여러 특징을 독립적으로 학습할 수 있게 된다. 

-> 신경망은 데이터에서 다양한 패턴을 효율적으로 추출하고 학습할 수 있게 된다. 
-> 결론 : ReLU 는 참 좋은 놈이다.

## Batch Normalization
"공변량 시프트 현상을 누그러뜨리기 위해"

공변량 시프트가 뭔지도 모른다.
- 공변량 시프트란? 훈련 중에 입력 데이터의 분포가 변화하는 현상(깊은 구조의 신경망 학습이 잘 되지 않는 이유 중 하나)
- 예를 들어, 훈련 초기에는 입력 데이터의 평균값이나 표준편차가 일정했지만, 학습이 진행될수록 그 값이 달라져 모델이 적응하기 어려워짐.

배치 정규화란?
- 각 미니배치마다 출력값을 정규화하여, 각 층의 입력 분포가 일정하게 유지되도록 돕는 것
- 이는 각 층에 들어가는 데이터가 평균 0, 표준편차 1을 갖도록 정규화하는 방식임.
- 따라서 아래의 식을 모든 층에 독립적으로 적용한다

![alt text](img/딥러닝_최적화/image17.png)


 알고 있어야 하는 수식
![alt text](img/딥러닝_최적화/image18.png)

평균과 표준편차의 흔한 수식인데, 이를 미니배치마다 계산해주어야 한다는 게 포인트

최적화를 마친 후에는 노드에 평균, 분산, γ, β 를 저장해야 하므로, 노드의 개수가 100개라고 하면
400개를 저장하고 있어야한다. 그리고 예측 단계에서 각 노드는 독립적으로 아래의 식을 적용한다.

![alt text](img/딥러닝_최적화/image19.png)

CNN에서는 노드 단위가 아닌 특징 맵 단위로 코드를 적용한다. 

배치 정규화의 장점
- 가중치 초기화에 덜 민감하다
- 학습률을 크게 하여 수렴 속도 향상 가능
- 시그모이드를 활성함수로 사용하는 깊은 신경망도 학습이 이루어진다
- 드롭아웃이라는 규제 기법을 적용하지 않아도 높은 성능을 보인다


# 과잉적합에 빠지는 이유와 과잉적합을 피하는 전략
![alt text](img/딥러닝_최적화/image20.png)

사진을 보아 훈련집합의 경우, 모델용량이 작을 수록 오류율이 높고, 클 수록 오류율이 낮다.
다만 테스트 집합의 경우, 일정수준을 넘어가게 되면 훈련집합의 과잉적합으로 인해 일반화 차이가 발생하는 것을 볼 수 있다.
따라서 현대 기계 학습은 충분히 큰 용량의 신경망 구조를 설계한 다음, 학습 과정에서 여러 규제 기법을 적용하는 전략을 사용한다.

## Weight Penlaty
![alt text](img/딥러닝_최적화/image21.png)

목적함수들은 가중치 집합과 훈련집합애 영향을 끼치지만,
규제항 R은 훈련 집합과 무관하다.(가중치에만 영향)

R은 단지 가중치의 크기에 제약을 가하는 역할을한다. 즉, 데이터셋과 무관하게 원래있는 사전지식이다.
규제항은 매개변수를 작은 값으로 유지하므로 모델의 용량을 제한하는 역할을 한다.

큰 가중치에 벌칙을 가해 작은 가중치를 유지하기 위해 주로 L2 Norm 이나 L1 Norm 을 사용한다.

### L2 Norm
- 규제항 R로 L2 Norm 을 사용하는 규제기법을 가중치 감쇠(weight decay)라고 부른다. 식은 아래와 같다.
![alt text](img/딥러닝_최적화/image22.png)

- J regulized(θ;X,Y)는 정규화를 적용한 손실함수를 뜻
- J(θ;X,Y)는 원래의 손실 함수
- 뒤에 따로 있는 λ 식이 정규화 항으로, 가중치 벡터 θ = [wo, w1, w2]의 제곱합이다.
- λ는 정규화 강도를 조절하는 계수

결론적으로 가중치 값이 너무 커지는 것을 방지한다. 
-> 정규화 항은 손실 함수에 추가되는 '페널티'이다. 기존 손실함수에서 무언가를 더한다는 것은 페널티를 키운다는 뜻.
-> 따라서 가중치가 커지면 페널티값도 커져서 학습 과정에서 가중치를 작게 만드는 방향으로 업데이트가 이루어진다.

#### 그레디언트 계산
기존 L2 Norm 식에서 미분을 때리면 아래와 같이 쓸 수 있고
![alt text](img/딥러닝_최적화/image24.png)

결과적인 가중치 업데이트 식은 아래처럼 쓸 수 있다.
![alt text](img/딥러닝_최적화/image25.png)

즉, 기존의 그레디언트값에 2λwi(페널티 항) 를 더한다.
이렇게 하면 모델이 덜 복잡해지고, 일반화 성능이 좋아지는 것

#### 매개변수 갱신 수식
기본적으로 경사하강법에서 가중치 업데이트 공식은 다음과 같다.
![alt text](img/딥러닝_최적화/image26.png)

위에서 배운 L2 Norm을 적용시키면 가중치 업데이트 공식은 다음과 같고
![alt text](img/딥러닝_최적화/image27.png)

이를 정리하면 아래의 식으로 최종 정리할 수 있다.
![alt text](img/딥러닝_최적화/image28.png)

이 식의 핵심은 (1-2ρλ)θ 부분인데, 여기서 λ 값이 0보다 크다면, 
1-2ρλ는 1보다 작은값이 되고, 결과적으로 1보다 작은값이 θ(가중치)와 곱해져 가중치가 줄어들게 된다.(ρ는 learning rate로 애초에 1보다 훨씬 작은 수를 사용)
L2 정규화 자체가 가중치를 줄이고 모델 복잡도를 억제하는 데 목적이 있으므로, λ 값은 항상 0 이상으로 생각하면 된다.(0인 경우 원래 경사하강법 그대로 적용)

결론적으로 λ 식이 추가됨으로 인해 최적화 과정에서 θ값이 작아지고, 결과적으로 0(원점)에 조금 더 가까워진다.
간단하게 설명해서, 가중치를 줄여서 일반화 성능을 향상 시키게 된다.
- 가중치가 적당히 작아지면, 모델은 덜 복잡하고 매끄러운 함수가 된다.
- 복잡하지 않은 모델은 데이터의 노이즈보다는 일반적인 패턴을 학습하려고 하기 때문에, 이는 새로운 데이터에서 더 나은 성능을 제공한다.

#### 선형회귀에 적용
일단 기본적인 선형 회귀의 목표 : 주어진 입력 데이터 X와 타겟 값 Y 사이의 관계를 나타내는 가중치 w를 학습하기

선형 회귀를 수식으로 표현하면 y^ = Xw 
- X : 입력 값(특성 행렬) -> 예 : 3x2 행렬인 경우 3개의 샘플에 2개의 특성을 가진 데이터
- w : 가중치 벡터
- y^ : 모델이 예측한 출력값

선형 회귀의 핵심은 실제 값 Y와 예측값 y^의 차이를 최소화 하는 것인데, 
오차는 다음과 같이 계산할 수 있다 'Xw-y' 
- Xw : y^ (예측 출력값)
- y : 실제 출력값(레이블)

이 오차를 제곱하고 더한 값이 MSE라고 불리던 놈이다. (기본 손실함수)
![alt text](img/딥러닝_최적화/image29.png)

이 놈에 L2 정규화 항 λ를 추가하여 아래의 식을 만든다
![alt text](img/딥러닝_최적화/image30.png)

자기 자신의 내적은 제곱합과 동일하다

위 식을 미분하여 0으로 놓고 정리하면 최종적으로 아래의 식이 된다.
![alt text](img/딥러닝_최적화/image31.png)

역행렬을 곱하므로 가중치를 축소하여 원점으로 당기는 효과.
예측 단계에서는 위에서 구한 최적의 매개변수값을 사용
![alt text](img/딥러닝_최적화/image32.png)


## Early Stopping
- 일정 학습 시간이 지나면 단순히 훈련 데이터를 암기하기만 하는 과잉적합 현상이 일어나기 때문에 일반화 능력이 저하될 수 있다.
- 이를 막기 위해 검증집합의 오류가 최저인 지점에서 학습을 멈춘다.

Early Stopping을 채택한 알고리즘은 두 가지 버전으로 볼 수 있는데.

첫 번째는 현실(지그재그 현상)을 제대로 반영하지 않은 순진한 버전이다.

![alt text](img/딥러닝_최적화/image33.png)

코드의 6번 라인에 따라 오류율 e가 조금이라도 증가하면 즉시 학습을 중단한다.
하지만 실제 학습 과정에서는 아래의 사진처럼 일시적으로 오류율이 증가하는 지그재그 현상이 발생할 수 있는데,
저 코드의 로직이라면 아래의 사진에서 t3 지점이 아닌 t1 지점에서 멈추게 될 것이다.

![alt text](img/딥러닝_최적화/image34.png)

따라서 위 사진의 t3에서 멈추게 하려면 아래의 알고리즘(지그재그를 반영한)을 사용해야 한다.

![alt text](img/딥러닝_최적화/image35.png)

일단 참을성이라는 건, 일시적으로 오류율이 증가해도 바로 멈추지 않고 일정 횟수 동안 더 기다려보는 과정을 말한다.
q=3 으로 설정한 것은, 3세대 동안 성능이 나빠지더라도 계속 학습을 진행하겠다. 라는 의미

동작 과정
- 오류율이 가장 낮은 e^을 기준으로 삼는다.
- t 세대부터 q 번 반복을 거쳐 업데이트된 매개변수의 오류율(e_t+q)이 기존 최저 오류율 e^보다 좋지 않다면 j 값을 1 증가시킨다.
- 만약 e_t+q 가 기존 e^보다 좋다면 오류율을 새로 기록하고, 참을성 횟수인 j를 0으로 초기화 시킨다.
- j 값이 지정된 수치에 도달하면 종료

총 학습 허용 세대 수 계산
- q=3, j=4 라면, 4*3 = 12
- 학습이 나빠져도 최대 12세대 동안 기다리며 개선될 기회를 준다는 뜻


## 데이터 확대 (Data Augmentation)
과잉적합을 방지하는 가장 확실한 방법은 큰 훈련집합을 사용하는 것이다.
하지만 데이터 수집은 비용이 많이 드는 작업이기에 인위적으로 이를 변형하여 개수를 늘릴 수 있다.

이동, 회전, 크기 변환 등을 적용하는 것은 모두 Affine 변환에 속하고 (선형 변환에 평행 이동을 추가한 것, 여기서는 선형 변환이라고 봐도 무방)

모핑을 이용한 비선형 변환도 있다.

![alt text](img/딥러닝_최적화/image36.png)

모핑은 같은 부류에 속하는 두 샘플 사이의 변환(interpolation) 과정을 자동으로 알아낸 후, 
이 변환을 통해 중간 상태의 데이터를 생성함으로써 새로운 데이터를 더 자연스럽게 만들어내는 기법

Affine 변환에 비해 훨씬 다양한 형태의 확대가 가능하다.

이외에도 영상을 자르 좌우반전과 색상 변형(PCA)을 통해 데이터를 증가시키는 기법과, 
입력 데이터와 은닉 노드에 잡음을 석는 기법 등이 존재한다.

## 드롭아웃

### 학습단계

- 학습 과정 중 임의로 노드의 일부를 제거하고 연결을 끊는다. 
- 이를 통해 노드들은 특정 노드에 의존하지 않도록 만들어 새로운 환경에서도 적응할 수 있도록 만든다.
- 즉, 과적합을 방지하면서 일반화 성능을 높인다.

입력층과 은닉츠의 노드를 제거하는 것임.

![alt text](img/딥러닝_최적화/image37.png)

위 알고리즘의 대한 설명은 아래와 같다.

1. 모델의 가중치 θ를 난수로 초기화
2. 반복문을 통해 최적화가 완료될 때까지 학습 진행
3. 학습 데이터에서 미니배치 B를 샘플링
4. 입력층과 은닉층에 설정한 비율로 드롭아웃을 수행(즉, 입력 노드와 은닉 노드의 일부를 랜덤하게 비활성화)
5. 드롭아웃이 적용된 부분 신경망을 사용해 전방 계산 (출력 계산)
6. 오류 역전파를 통해 드롭아웃된 신경망에 대한 그레디언트를 계산
7. 모든 샘플의 평균 그레디언트를 구함
8. 평균 그레디언트를 바탕으로 가중치 θ 업데이트
9. 학습이 멈추는 조건에 도달하면 최종 최적화된 가중치 θ를 반환

![alt text](img/딥러닝_최적화/image38.png)

알고리즘 이미지의 라인 6에 관한 자세한 설명
- 먼저 불리언 배열 π는 0 또는 1로 구성된 참/거짓 배열이다
- 각 값이 1이면 노드를 활성화하고, 0이면 노드를 비활성화(제거) 한다.

![alt text](img/딥러닝_최적화/image39.png)

위와 같은 배열에서 첫 번재와 네 번째 노드가 살아남고, 두 번째와 세 번째 노드가 꺼진다.

![alt text](img/딥러닝_최적화/image40.png)

![alt text](img/딥러닝_최적화/image41.png)

π 는 각 노드를 켜거나 끄는 스위치라고 생각할 수 있다. (드롭아웃을 적용하는)

### 예측단계

예측 단계의 핵심은 가중치 조정이다. 학습 과정에서 드롭아웃이 적용되었으므로, 예측 단계에서는 모델의 가중치를 보정해야 한다.
- 드롭아웃 비율이 p라면, 생존 비율은 1-p
- 따라서 예측 단계에서 각 가중치 U에 1-p 를 곱해서 보정한다.

![alt text](img/딥러닝_최적화/image43.png)

왜 이렇게 하나요?
- 드롭아웃은 학습 과정에서 일부 노드만 참여하였으므로, 전체 노드가 활성화되는 예측 단계와의 차이를 보정해야 하기 때문
- 1-p를 가중치에 곱해주게 되면, 학습과 예측의 평균 출력값이 일치하게 된다.

드롭아웃을 통해 생기는 추가 메모리는 불린 배열 π 뿐이다.
실제 메모리 부담은 신경망의 크기(노드와 가중치 수)에 의해 결정된다.


## 앙상블

앙상블 : 여러 개의 모델을 결합하여 최종 예측 성능을 높이는 방법

앙상블은 다음과 같은 두 가지의 일을 수행한다.
1. 서로 다른 예측기를 학습하는 일
 - 서로 다른 신경망 구조를 사용하거나 초기값과 하이퍼파라미터를 다르게 설정
 - 배깅 : 데이터를 여러 번 샘플링해 다른 훈련 집합을 구성하는 것 (예 : 랜덤 포레스트)
 - 부스팅 : 이전 예측기가 틀린 부분에 집중하여 다음 예측기를 학습시키는 방법 (예 : AdaBoost, Gradient Boosting)

2. 학습된 예측기를 결합하는 일
 - 주로 투표 방식을 사용
 - 각 모델이 내놓은 결과 중 다수결 투표로 최종 결과를 결정

![alt text](img/딥러닝_최적화/image42.png)