# 목적함수

강의 자료 첫 부분에 이런 글이 나온다.

"시험에서는 틀린 만큼 합당한 벌점을 받는 것이 중요하다. 그래야 다음 시험에서 심기일전으로 공부하여 틀리는 개수를 줄일 가능성이 크기 때문이다. 틀린 개수에 상관없이 비슷한 발점을 받는다면 나태해져 성적을 올리는 데 지연이 발생할 것이다. 이러한 원리가 기계 학습에도 적용될까?"

바로 다음 페이지에 평균제곱 오차(MSE)가 나오는데, 이것이 벌점과 같은 작용을 하는 듯 하다.

평균제곱 오차는 다음과 같이 나타낸다.
![alt text](img/딥러닝_최적화/image.png)

y(정답값)와 o(예측값)의 차이가 클 수록 e 값이 커지므로, 벌점의 형태로서 적합하다고 볼 수 있다.

하지만 큰 허점이 존재한다.

![alt text](img/딥러닝_최적화/image1.png)

위 사진의 경우, 왼쪽 그림이 오른쪽 그림보다 정답값-예측값의 크기가 작으므로, 왼쪽그림의 벌점이 더 적게 나와야 정상이다.
하지만 계산을 해보니 오른쪽 신경망의 gradient 값이 더 작다.
gradient 값이 작다는 것은 '모델 파라미터를 수정할 필요가 적다는 것'을 의미한다.
벌점은 더 많이 받으면서 수정할 필요가 적은 모순적인 상황이 발생하기 때문에 이 녀석(MSE)은 목적함수로서 부적절하다고 판단할 수 있다.

## 교차 엔트로피 목적함수

Cross Entropy 라고 하는 놈인데 실제로 여러 학습 코드를 돌리면서 상당히 자주 본 친구다.
이 녀석은 적합한 목적함수임이 틀림 없을 것이다.

그 전에 먼저 엔트로피란 무엇인가?

Entropy : 정보를 표현하는 데 필요한 최소 자원량(bits의 단위로 표현 되는 0 or 1의 길이) 즉, 최소 기댓값

예시로.. 친한 친구와의 카톡 대화에서 'ㅋ'을 상대적으로 많이 치니까 P(ㅋ)=0.5 , P(술)=0.3 , P(공부)=0.01 정도로 잡을 수 있겠다.
그러므로 많이 치는 'ㅋ'을 짧 코딩하고(예: 0으로 표현) '공부'를 길게 코딩해야(예: 111로 표현) 효율적일 것이다.
요약하면, "확률이 작을수록 길이가 길어진다." 
![alt text](img/딥러닝_최적화/image2.png)

설명을 그래프로 표현하면 위 사진과 같고 이는 -log 그래프 형태를 띈다.
bit로 표현해야 되기 때문에 log의 밑은 2가 되고, 확률로써 표현해야 되기 때문에 -log2(Pi)가 되며,
기댓값을 구하기 위해 각 길이(도수)와 확률을 곱한 값을 전부 더해주어야 한다.
길이는 -log2(Pi) 인 것이고, 확률은 Pi 이므로 최종 식은

![alt text](img/딥러닝_최적화/image3.png)


하지만 Cross Entropy 에서는 이 확률들을 모두 균등하게 잡는다. P는 실제확률이고, 내가 생각한 확률은 Q로서 표현한다 (Q(ㅋ)=0.01, Q(술)=0.01, Q(공부)=0.01)
나머지 부분은 Entropy와 동일하고 P대신 Q로 바꿔주면 되기 때문에 Cross Entropy의 최종식은 Entropy 식에서 P만 Q로 바꿔주면 된다.

이를 벌점으로서 유용한지 확인하기 위해 아래 식에 대입해보면

![alt text](img/딥러닝_최적화/image4.png)

정답값과 예측값의 차이가 클 때 더 큰 벌점을 부여하는 것을 확인. 1단계는 통과했고,
그레디언트까지만 통과하면 목적함수로서 적합하다고 할 수 있다.

미분과정을 생략하고 표현하면

![alt text](img/딥러닝_최적화/image5.png)

로 나타내지고, 값을 대입해보면 그레디언트도 벌점에 비례하여 나타나는 것을 확인하였다.

결론 : Cross Entropy는 목적함수로서 적합하다!


## Softmax

Multi Classification 을 위해 사용하는데, 상대 평가가 가능하며 무조건 1보다 작고, 각 확률의 합은 1이다.
식으로 나타내면 아래와 같다.

![alt text](img/딥러닝_최적화/image6.png)

![alt text](img/딥러닝_최적화/image7.png)

Sigmoid와는 달리, softmax는
최대값을 더욱 활성화하고 작은 값을 억제하는 효과가 있다. 이는 목적에 따라 유용하게 쓸 수 있는데, 예를 들어 softmax의 출력에 따라 가장 큰 값과 두 번째 큰 값의 차이가
임곗값보다 작으면 분류를 포기하고 기각하는 전략을 쓸 수 있다.

강의 자료에 그렇게 많은 내용이 있지 않으니, 여기까지 하고 패스



# 성능 향상

## 데이터 전처리

규모 문제에 대해 다뤄보자.

사람 2명에 대한 데이터가 있다고 치자.

키 : 1.855m , 1.525m
몸무게 : 65.5kg, 45.0kg

두 사람에 대한 키 차이는 0.33 정도
두 사람에 대한 몸무게 차이는 20 정도

단위 때문에 두 특징값 차이가 70배 정도 발생한다.
이렇게 되면 첫 번째 특징에 연결된 가중치는 두 번째 특징에 연결된 가중치에 비해 70배 정도 느리게 학습되는 것이다.

또한, 위처럼 모든 특징이 양수인 경우에도 문제가 발생한다.

이는 경사 하강법에서 적용 되는 내용인데, 경사 하강법은 손실 함수를 최솟화하기 위해 사용한다.
여기서는 기울기와 학습률이 영향을 미치는데,
만약 모든 특성이 양수라면 기울기 변화가 너무 급격하게 일어날 수 있어 이를 적절히 조절하지 않으면 학습이 느리게 이루어질 수 있다.


## 표준화

규모 문제와 데이터 양수 문제를 해결해주는 것이 표준화이다.

표준화는 데이터를 평균 0, 분산 1로 변환하는 방식이다.

![alt text](img/딥러닝_최적화/image8.png)

## 정규화

데이터를 0과 1 사이의 범위로 변환하는 방식. 주로 최솟값과 최대값을 사용하여 반환

![alt text](img/딥러닝_최적화/image9.png)

데이터셋이 [1,2,3,4,5] 일 때 대입하면 모두 0과 1 사이(0과 1 포함) 범위로 조정 됨. => 규모 문제 해결

## 원 핫 인코딩
키, 몸무게 같은 데이터와 달리 성별이나, 체질(예: 태양인, 소양인 등..)은 거리 개념(물리적 크기)을 갖지 않는다.
따라서 남자가 여러명일 때 남자 1, 남자 2 이런 식으로 표현하는 게 아니라. 성별의 경우 남 녀 2개 뿐이니까 
남자면 1 0  여자면 0 1 이런 식으로 표한한다. -> 값의 개수만큼 비트를 부여한다. 

이는 수학적으로는 값이 다를 수 있지만, 그 차이가 실제 크기나 거리 개념을 의미하지 않는다라는 의미를 전달한다.

## 가중치 초기화
신경망의 가중치는 난수를 생성하여 초기화해야 한다.

이유 : 신경망에서 모든 가중치를 동일한 값으로 초기화하면, 모든 뉴런들이 동일한 값을 학습하게 되어 '대칭이 깨지지 않는다'
-> 즉, 각 뉴런이 서로 다르게 학습할 기회가 없어진다. 따라서 난수 초기화는 각 뉴런의 가중치를 다르게 설정하여 각각 다른 패턴을 학습할 수 있도록 돕는다.

가중치 초기화 시, 가우시안 정규분포 혹은 균일분포에서 값을 추출하는 데 둘 중 어떤 것을 사용해도 성능 차이는 거의 없다.
중요한 것은 초기화의 범위 설정인데,

![alt text](img/딥러닝_최적화/image10.png)

너무 작은 값으로 초기화 하면, 0으로 가까운 값으로 시작하니까 정보가 사라지거나 학습이 잘 안 될수 있다
너무 큰 값으로 초기화 하면, 값이 폭주하거나, 네트워크가 불안정해져서 학습이 어려워진다.

따라서 위 사진은 '최적의 범위를 제공하는 초기화 범위 계산 공식' 정도로 알아두면 되겠다.

bias는 보통 0으로 초기화하며, 대표적으로 AlexNet 과 ResNet 에서 이를 사용하였다. 

## 모멘텀
그레디언트에 '스무딩'을 가하여 잡음 효과를 줄인다 -> 수렴 속도 향상의 효과
![alt text](img/딥러닝_최적화/image13.png)

속도 벡터 v는 이전 그레디언트를 누적한 것에 해당한다.(처음 v는 0)
알파값의 효과는 알파가 0이면 적용이 안 된 것과 같고, 
알파가 1에 가까울수록 이전 그레디언트의 가중치를 많이 사용하는 느낌. 통상적으로 0.5, 0.9, 0.99를 사용한다.


![alt text](img/딥러닝_최적화/image11.png)

검은선이 모멘텀을 적용하지 않은 것이고, 파란선이 모멘텀을 적용한 것이다.
검은선을 보면 이동량이 너무 커서 적절한 곳을 지나치는 'Overshooting'이 발생하지만, 
파란선의 경우 Overshooting 현상이 감소하였고, 검은선에 비해 적은 반복 횟수로 최적해를 찾아간다. 

### 네스테로프 모멘텀
![alt text](img/딥러닝_최적화/image12.png)

현재는 기존 모멘텀 기법을 개선한 '네스테로프 모멘텀'이 널리 사용되고 있다.
현재 v 값으로 다음 이동할 곳을 예견한 후, 예견한 곳의 그레디언트를 사용하는 것.


## 적응적 학습률
![alt text](img/딥러닝_최적화/image14.png)

학습률이 너무크면 검은선 처럼 오벼슈팅 현상이, 
학습률이 너무 작으면 파란선 처럼 수렴이 느린 현상이 발생한다.
단순히 그레디언트에 학습률 p를 곱하면 모든 매개변수가 같은 크기의 학습률을 사용하는 셈이라 최적화가 되지 못한다.
따라서 적응적 학습률은 매개변수마다(각 세타별로) 자신의 상황에 따라 다른 학습률을 조절해서 사용한다; [학습률 답금질]

AdaGrad(단순히 제곱을 더함) -> RMSProp(가중치 이동 평균) -> Adam(RMSProp + Momentum)


## Activation Function
활성값 z를 계산하고 활성함수 τ를 적용하는 과정

![alt text](img/딥러닝_최적화/image15.png)

![alt text](img/딥러닝_최적화/image16.png)

tanh의 경우 활성값이 커지면 포화상태가 되고 그레디언트가 0에 가까워지는데, 
이는 매개변수 갱신이 매우 느린 요인이 된다.

가장 오른쪽에 나와있는 그래프인 ReLU는 이러한 포화문제를 해소한다.
좀 더 쉽게 설명하자면, 면 양의 값에서는 선형이기 때문에 큰 값에서 기울기가 너무 작아져서 학습이 느려지는 문제가 없다.
반면, 음의 값에서는 출력이 0이 되기 때문에 희소한(신경망에서 많은 뉴런 또는 가중치가 0이 되는 상태) 신경망을 만든다.
예를 들어, 신경망이 처음에 무작위로 초기화되면, 대략 절반 정도의 뉴런에서 입력 값이 음수가 될 수 있다.
이 때, 음수인 뉴런들은 모두 출력이 0이 되어 학습에 영향을 미치지 않게 되는데(계산에서 제외됨), 이렇게 되면 신경망이 희소한 상태가 되어,
각 뉴런이 특정 특징을 담당하게 되며, 여러 특징을 독립적으로 학습할 수 있게 된다. 
-> 신경망은 데이터에서 다양한 패턴을 효율적으로 추출하고 학습할 수 있게 된다. 
-> 결론 : ReLU 는 참 좋은 놈이다.





