## 문제점 요약
- 자연영상 분류란 다양한 실생활 이미지를 분석하고 분류하는 문제를 말함
- 다양한 각도와, 배경 또는 수많은 부류 등 자연영상 분류에는 여러 가지 도전 과제가 있다.
- 이러한 문제를 해결하기 위해 ImageNet 데이터베이스와 ILSVRC 대회가 등장하였다.
- ImageNet : 약 2만 개의 클래스로 나누어진 대규모 이미지 데이터셋
- ILSVRC 대회는 1000개의 부류의 객체를 분류하고 검출하는 대규모 도전 과제를 제시하였음

대회는 다음과 같은 규모를 자랑한다
- 훈련집합 : 120만 장의 이미지
- 검증집합 : 5만 장
- 테스트집합 : 15만 장

2012년부터 2015년까지 우승을 했던 신경망에 대해 알아볼 것(우승한 CNN은 프로그램과 가중치를 공개함으로써 널리 사용되는 표준 신경망이 됨)

## AlexNet

2012년도에 우승을 차지한 AlexNet이다.

### AlexNet 구조

![alt text](img/CNN_발전사례/image.png)

5개의 컨볼루션 층(C1~C5)과 3개의 완전연결층으로 이루어져있다.

각 층의 역할은 다음과 같다.
1. 입력 : 224x224x3 크기의 이미지

2. 컨볼루션 층
 - C1 : 11x11 필터를 사용해 특징 추출 
   - 필터 개수 : 96개 -> 출력 크기 : 55x55x96 , stride=4
 - C2 : 필터 개수 256개 -> 출력 크기 : 27x27x256
 - C3~C5 : 점점 더 많은 필터를 사용하여 이미지에서 더 세부적인 특징을 추출함.

3. Max Pooling
 - 각 컨볼루션 층 다음에 Max Pooling을 적용한다 (2x2사이즈, stride=2)
 - 이는 특징맵을 줄여 계산량을 줄이고 중요한 특징만 남기는 역할을 한다

4. 완전 연결층
 - 출력된 특징을 1차원 벡터로 펼쳐서 연결한다.
 - FC1, FC2 : 4096개의 노드
 - FC3(최종 출력층) : 1000개의 노드 (1000개의 클래스 예측)

5. GPU 병렬 처리
 - AlexNet은 학습 시 2개의 GPU를 사용하여 계산을 나누어 병렬로 처리하엿다.
 - 덕분에 큰 모델임에도 빠르게 학습할 수 있었다.

### AlexNet 성공 이유

1. 외부 요인
 - 대용량 데이터셋(ImageNet)
 - 2개의 GPU 병렬 처리

2. 내부 요인
 - ReLU 활성 함수 사용 (기존 Sigmoid에서 바꿈으로써 느린 계산 문제와 기울기 소실 문제 완화)
 - 지역 반응 정규화 : LRN 기법을 사용해 뉴런 간의 경쟁을 유도하고 학습을 돕는다
 - 규제 기법 : 데이터 확대, 드롭아웃

테스트 단계에서는 여러 이미지 변형을 생성하고 각 변형의 결과를 앙상블하여 최종 예측을 수행했다.
결과적으로 2~3%의 오류율 감소 효과를 얻음.


## VGGNet

![alt text](img/CNN_발전사례/image1.png)

1. 핵심 아이디어
 - 3x3 작은 커널을 사용해서 깊은 네트워크를 구성
 - AlexNet의 5개 층보다 2~3배 깊은 구조

2. VGG-16 구조
 - 총 16층 : 13개 컨볼루션 층 + 3개 완전 연결층

3. 전달 과정
 - max pooling으로 인해 사이즈는 절반으로 계속 줄어들면서, 채널수는 2배씩 증가함

4. 활성화 함수
 - ReLU와 같은 비선형 함수가 적용되며 Softmax로 최종 분류 수행


### 전체 특징

1x1 커널의 역할 : 차원축소, 비선형성 추가, 연산량 감소

입력 특성맵 mxnx8 에 대해 1x1x8 크기의 커널 4개를 적용하면 출력 텐서는 mxnx4가 된다.


3x3 커널의 역할 : 작은 커널을 여러 번 적용해 수용 영역을 키우고 깊이를 늘림

층이 깊어질수록 필터 수는 점진적으로 늘어남 (64->128->256->512)


## GoogLeNet

![alt text](img/CNN_발전사례/image2.png)

NIN이란? 
- Network In Network, 기존의 컨볼루션 레이어 대신에 작은 다층 퍼셉트론(MLP)을 사용하는 구조
- 여기서 MLP는 작은 완전 연결층 레이어를 사용하여 로컬 영역의 특징을 더 잘 학습하도록 도움

기존 CNN과는 달리 NIN에서는 컨볼루션 후에 바로 작은 MLP를 적용한다.
이는 각 로컬 영역에서 추가적인 비선형 변환을 수행하여 더 복잡한 특징을 학습하는 것을 돕는다.

MLPconv 층은 1x1 컨볼루션과 유사하지만, 각 위치에 대해 다층 퍼셉트론을 적용한다.

### 그림 설명

[왼쪽 그림]
1. 3x3 필터를 사용해 이미지의 로컬 특징을 추출한다.
2. 채널 수는 변하지만, 복잡한 비선형 변환이 부족하다.

[오른쪽 그림]
1. 3x3 필터 뿐만 아니라 MLPconv 층을 도입함
2. MLPconv는 작은 완전 연결층 레이어를 통해 각 위치에서 더 깊은 변환을 수행함

### NIN이 사용하는 전역 평균 풀링

VGGNet과 같은 전통적인 CNN 모델은 완전 연결층 레이어(FC)를 사용한다.
하지만 FC 층은 매개변수가 매우 많고 이로인한 과적합 위험과 학습이 느린 문제가 있다.

따라서 GoogLeNet의 해결책은 전역 평균 풀링(GAP)이다.
- 전역 평균 풀링 : 각 특성 맵의 값을 평균 내어 1개의 값으로 요약함.
- 이를 통해 FC 층을 완전히 제거할 수 있었다.

### 인셉션 모듈

인셉션 모듈은 NIN 아이디어를 확장한 구조로, 다양한 크기의 필터를 동시에 사용해 효율적으로 특성을 학습한다.

1. 인셉션 모듈이란?
- 여러 종류의 컨볼루션 연산을 동시에 사용해 다양한 특성을 추출함
- 1z1, 3x3, 5x5 컨볼루션과 풀링 연산을 한 번에 적용하고 결과를 합친 후 다음 단계로 전달함.
- 다양한 크기의 특징을 동시에 학습하고, 차원 축소를 통해 계산량을 줄이는 효과

![alt text](img/CNN_발전사례/image3.png)

2. 구조 이해
 1) 1x1 컨볼루션
  - 차원 축소 역할을 함
  - 입력 채널 수를 줄여서 다음 단계의 연산량을 줄이는 데 사용

 2) 3x3 컨볼루션
  - 중간 크기의 특징을 추출함
  - 1x1 컨볼루션을 먼저 적용해 채널을 줄이고, 3x3 필터로 특성을 학습함

 3) 5x5 컨볼루션
  - 더 큰 수용 영역의 특징을 학습함
  - 연산량이 크기 때문에 1x1 컨볼루션을 먼저 사용해 채널을 줄인 후, 5x5 필터를 적용

 4) 3x3 맥스 풀링
  - 공간 정보를 축소하면서 중요한 특징만 남김
  - 이후 1x1 컨볼루션을 사용해 채널 수를 맞추고 결과를 합침

정해진 p, s, h, k 값들을 통해 입력과 출력의 크기를 동일하게 유지하고, 최종 채널수를 모두합하여 480이라는 결론에 도달한다.


![alt text](img/CNN_발전사례/image4.png)

### 9개의 인셉션 모듈 결합
- 총 27층으로 구성
- 22개 층 : 학습 가능한 파라미터를 가진 레이어(컨볼루션, FC)
- 5개 층 : 파라미터 없는 레이어(풀링)
- 9개의 인셉션 모듈이 전체 네트워크를 구성하며, 각 모듈은 다양한 크기의 필터와 풀링 결과를 결합한 출력을 생성함

#### 매개변수 절감
- VGGNet은 완전연결층에서 약 1억 개의 매개변수를 사용함
- GoogLNet은 이를 제거하고 전역 평균 풀링을 사용해 1백만 개의 매개변수만 사용 (VGGNet의 1% 수준)

#### 보조 분류기 사용
- 네트워크의 중간 단계에 보조 분류기를 두어 학습을 돕는다
- 네트워크가 너무 깊으면 기울기 소실 문제가 발생하기 때문
- 중간 출력을 softmax를 통해 분류
- loss를 추가로 계산하여 역전파를 통해 학습 가속화


