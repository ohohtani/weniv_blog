# 정책 기반 강화학습

- 순차적 행동 결정 문제에 접근하는 방식
- 가치함수를 토대로 행동을 선택하지 않고, 상태에 따라 바로 행동을 선택
- 정책을 직접적으로 근사시킴
- 정책 기반 강화학습에서는 인공신경망이 정책을 근사함.

정책을 직접 학습하여 행동을 선택하는 방법으로, 복잡한 문제에서도 더 효과적으로 적용할 수 있으며, 신경망을 통해 더욱 정교한 정책을 학습할 수 있다.

정책신경망(정책 기반 강화학습에서 정책을 근사하는 인공신경망)에서는 활성함수로 softmax를 사용한다.
cuz, 정책의 정의가 바로 각 행동을 할 확률이기 때문에 이 확률의 합은 1이 되어야 하므로

누적 보상은 정책신경망의 가중치에 따라 결정된다.

정책신경망으로 정책을 대체함 -> θ라는 정책신경망의 가중치 값이 정책을 표현할 수 있음

누적 보상을 최대화 하는 것이 목표함수가 되며, 최대화를 하게 되는 변수가 인공신경망의 가중치임.

## 정책 기반 강화학습의 목표

- 목표 함수 최대화 -> 이는 목표 함수를 미분하고 그 미분값에 따라 정책을 업데이트 하면 됨
- 딥살사와는 달리 오류함수를 최소화하는 것이 아니라, 목표함수를 최대화 하는 것
- 즉, 경사가 올라가야함(경사 상승법) 

![alt text](img/딥러닝과_강화학습/image.png)

J(θ)가 목표함수 인데 이는 𝐽(𝜃)=𝑣(𝜋0) (𝑠0)로 나타내며 이를 미분하면
여러 형태를 거쳐 모양이 변형되고 최종적으로 기존의 policy gradient 업데이트 수식은 아래 사진과 같이 정리된다.

![alt text](img/딥러닝과_강화학습/image1.png)