# RNN 쉽게쉽게 풀어쓰기

## 순환 신경망

먼저 RNN이란?
-> Recurrent Neural Network의 줄임말, 순서를 가진 데이터(시퀀스)를 잘 처리할 수 있는 신경망.
예를 들어 '문장'은 단어들이 순서대로 나열된 것이므로 RNN이 잘 작동한다. (입력도 문장, 출력도 문장이므로)

일반적인 신경망은 정보가 한 방향(입력 -> 출력)으로만 흐르지만, RNN은 자기 자신한테도 정보를 보낸다.
즉, 어떤 시점에 계산된 결과를 다음 시점의 입력으로도 사용한다. so, 과거 정보를 기억하는 것이 가능하다.

이걸 위해 셀(cell)이라는 장치를 쓰는데, 이것은 과거 정보를 담아두는 메모리 역할을 한다.
and 셀이 다음으로 전달하는 값을 '은닉 상태'라고 부름

예를 들어 t시점의 셀은:
-> 지금 입력, 바로 전 은닉 상태(ht-1) 이 두가지를 가지고 계산하여 지금의 은닉상태 ht를 만들고 이것을 출력하거나 다음 셀에 넘긴다

![alt text](img/RNN/rnn구조.png)

RNN은 입력과 출력의 길이를 자유롭게 정할 수 있어서 여러 작업에 활용된다.

One to Many (하나 -> 여러 개)
- ex) 이미지 한 장에서 문장 출력 (이미지 캡션 만들기)
Many to One (여러 개 -> 하나)
- ex) 문장을 보고 긍정/부정 판단하기
Many to Many (여러 개 -> 여러 개)
- ex) 문장을 다른 언어로 번역, 챗봇, 개체명 인식 등

[수식 이해하기]
![alt text](img/RNN/수식이해.png)

--- 대충 나와있어서 다시할게요 ---

RNN 입력/출력 차원
- 입력 x의 차원 : input_dim
- 은닉 상태 ht의 차원 : hidden_units
- 출력 y의 차원 : 문제에 따라 다름 (예: 이진 분류면 1)

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN

model = Sequential()
model.add(SimpleRNN(3, input_shape=(2, 10)))
```

3은 은닉 상태의 크기이며, 2는 시점의 수(timesteps) 
10은 입력 벡터의 크기이다.

#### RNN 직접 구현해보기
```python
import numpy as np

inputs = np.random.random((10,4))   # 시점 10개, 입력 차원 4
hidden_state = np.zeros((8,))       # 은닉 상태 크기 8로 초기화

Wx = np.random.random((8,4))        # 입력 가중치
Wh = np.random.random((8,8))        # 은닉 상태 가중치
b = np.random.random((8,))          # 편향

for x in inputs:
    output = np.tanh(np.dot(Wx, x) + np.dot(Wh, hidden_state) + b)
    hidden_state = output
```
매 시점마다 입력과 은닉 상태를 가지고 현재의 은닉 상태를 구한다
결과는 과거 정보를 반영한 벡터가 된다


#### 양방향 RNN
```python
from tensorflow.keras.layers import Bidirectional

model = Sequential()
model.add(Bidirectional(SimpleRNN(128, return_sequences=True), input_shape=(10,5)))
```
앞 + 뒤 방향 정보를 모두 사용하여 더 정확한 예측을 한다 ex) 빈칸 채우기


## LSTM에 대하여

### 바닐라 RNN의 한계

바닐라 RNN : 가장 기본적인 RNN
RNN은 앞 시점의 정보를 기억하면서 다음 시점에 영향을 주지만, 뒤로 갈수록 정보가 점점 희미해진다. (장기 의존성 문제)

![alt text](img/RNN/바닐라.png)

바닐라 RNN은 아주 단순하게 위와 같이 작동한다
- 현재 입력 xt + 이전 은닉 상태 ht-1 -> tanh 함수 통과 -> 현재 은닉 상태 ht

이 은닉 상태 ht는 출력으로도 쓰이고, 다음 시점의 입력으로도 넘어간다.
하지만 이 구조에서는 tanh 같은 함수를 지나면서 정보가 작아지고 흐릿해지기 때문에 기억력이 약해진다.

### LSTM : 기억력 좋은 RNN

핵심 아이디어는 기억해야할 건 기억하고, 잊을 건 잊자! 마인드 
RNN에 3개의 게이트를 추가하여 지울 것, 저장할 것, 내보낼 것을 정해서 정보를 똑똑하게 다룬다.

![alt text](img/RNN/lstm.png)

구성 요소 
- 셀 상태(ct) : 진짜 장기 기억, 과거 정보를 직접 전달하는 선
- 은닉 상태(ht) : 단기 출력, 다음 셀이나 출력층으로 가는 값
- 입력 게이트 : 지금 정보 중 뭘 기억할까?
- 삭제 게이트 : 이전 정보 중 뭘 지울까?
- 출력 게이트 : 뭘 꺼내서 출력할까?

### 게이트 하나씩 보기

![alt text](img/RNN/입력게이트.png)

입력게이트는 지금 들어온 정보 중, 뭘 기억해야할지 정한다.

현재 입력 xt + 이전 은닉 상태 ht-1 에 각각 가중치를 곱하고 편향을 더한 뒤, sigmoid를 씌워 얼마나 기억할지 결정한다 (0~1)
sigmoid 대신 tanh 를 씌우면 -1~1 의 범위가 된다.

![alt text](img/RNN/삭제게이트.png)

삭제게이트는 예전 기억 중에서, 무엇을 버려야할지 정한다

현재 입력 xt와 이전 은닉 상태 ht-1를 가중치 곱하고 시그모이드를 씌워 0~1의 값을 추출한다.
0에 가까울수록 정보가 많이 삭제된 것이고, 1에 가까울수록 정보를 온전히 기억한 것이다. 

![alt text](img/RNN/셀상태.png)

셀 상태는 실제로 기억을 유지하는 공간을 의미한다.

삭제게이트 ft와 이전 시점의 셀 상태를 곱하여 예전 기억의 일부를 삭제한다
(ft는 0~1 사이의 숫자 벡터이고 ct-1은 이전 시점의 셀 상태이므로 이전 시점의 셀 상태를 완전히 유지하거나 일부만 기억하는 것)

그리고 it와 gt의 원소곱한 결과를 더해주는데,
LSTM의 셀 상태 ct는 '과거 기억 중 유지할 부분'과 '지금 새로 저장할 부분'을 더한 것이므로
예전기억의 일부(ft*ct-1)와 지금 선택한 정보(itㅇgt) 를 더하여 셀 상태를 업데이트한다.

입력 게이트 it는 어떤 정보를 얼마나 기억할지를 조절하는 벡터이고,
기억 후보 gt는 기억하고 싶은 정보 자체이기 때문에,
이 둘을 '같은 위치끼리 곱'해서 '부분적으로 기억'하는 효과를 만드는 것이다.

![alt text](img/RNN/출력게이트.png)

출력 게이트는 현재 시점의 은닉 상태 ht를 만들기 위한 게이트이다.
즉, 셀 상태 ct에서 나온 정보를 바로 출력하지 않고, 필터를 한 번 더 거치는 것
그렇게 시그모이드를 거쳐 나온 값과 셀 상태(탄젠트를 거친)를 원소곱하여 은닉 상태 ht를 만든다

셀 상태를 탄젠트랑 왜 곱할까? 
-> 셀 상태 ct는 정보가 쌓여 있어서 값이 매우 클 수 있다. 그래서 직접 출력하지 않고 출력하기 전에 tanh를 씌워 -1~1 범위로 압축한다.

원소곱을 왜 할까?
-> 출력 게이트 ot는 '지금 셀 상태에서 어떤 정보를 얼마만큼 꺼낼지'를 위치별로 세밀하게 조절하기 위해 원소곱을 한다.

왜 덧셈과 행렬곱은 안되는가?
-> 원소곱을 하면 각 값 하나하나에 대한 별도 제어가 가능하다 (샤워기 구멍마다 벨브가 따로 있음, 어떤 구멍은 물을 다 내보내고, 어떤 건 잠그고, 어떤 건 절반만 가능)
-> 덧셈을 하면 게이트가 출력 양을 조절하는 역할을 못한다 (그냥 물의 세기만 높아짐, 어디로 나오는지는 조절 못함)
-> 행렬곱은 위치별로 꺼낼지 말지를 제어할 수는 없다 (구멍 위치를 바꾸거나, 수압을 섞을 순 있으나 출력 위치 자체만 바꿀 뿐, 꺼낼지 말지에 대한 판단을 못함)


## GRU에 대하여

GRU는 LSTM처럼 장기 기억 문제를 해결하면서도 구조를 더 간단하게 만들었다. 
LSTM과 비슷한 성능을 내면서도 빠르고 계산이 단순하다

![alt text](img/RNN/GRU.png)

LSTM에는 3개의 게이트(출력, 입력, 삭제)가 존재했던 반면, GRU에는 업데이트 게이트와 리셋 게이트 두 가지 게이트만 존재한다.
내부에는 은닉 상태 하나만 존재한다. (셀 상태 X)

업데이트 게이트(zt) : 얼마나 이전 정보를 유지할까? (LSTM의 삭제 + 입력 역할을 동시에 함)
리셋 게이트(rt) : 이전 정보를 얼마나 무시할까? 

GRU와 LSTM 중 어떤 것이 더 낫다고 판단할 수 없고, 만약 하파튜를 통해 LSTM의 최적 파라미터를 찾아낸 상황이라면 굳이 GRU로 안 바꿔도 된다.
데이터 양에 따라 유동적인 선택 (데이터가 많으면 GRU)

```python
model.add(GRU(hidden_size, input_shape=(timesteps, input_dim)))
```


## RNN 언어 모델

RNN 언어 모델(RNNLM)이 뭐냐
-> 단어들을 순서대로 입력해서 다음 단어를 예측하는 언어 모델이다.

what will the fat cat sit on 이라는 문장에 대하여
-> what 을 보면 -> will 을 예측하고
-> what will 을 보면 -> the 를 예측하고
-> what will the 를 보면 -> fat 을 예측하는 식으로 앞의 단어들을 바탕으로 다음 단어를 순차적으로 예측

이게 왜 필요해요? 
-> 기존의 n-gram 모델이나 NNLM은 고정된 개수의 단어만을 입력으로 받았다.
하지만 RNN은 시점이라는 개념이 있어서 입력의 길이가 달라도 상관 없이 처리가 가능합니다

훈련과 테스트 과정의 차이는 아래 사진과 같다

![alt text](img/RNN/훈련테스트차이.png)

훈련에서는 정답을 알려주면서 학습하고, 테스트에서는 모델이 스스로 예측한 결과를 다음 입력으로 사용한다.

### 교사 강요

훈련할 때 "네가 예측한 걸 쓰지 말고, 정답을 다음 입력으로 써!" 
-> 이유 : 예측이 틀리면 다음 입력부터 전부 틀릴 수 있으므로
-> 교사 강요를 사용하면 더 빠르고 안정적인 학습이 가능하다

![alt text](img/RNN/rnnlm.png)

1. 입력층 : 단어를 원-핫 벡터로 변환
2. 임베딩층 : 원-핫 벡터를 저차원 벡터(임베딩)로 변환
3. 은닉층(RNN) : 시점마다 이전 상태를 기억하며 계산
4. 출력층(소프트맥스) : 다음 단어를 확률 분포로 출력

ex) what will the fat cat sit on
-> 4번째 시점에서 fat을 입력으로 넣으면 은닉층은 앞의 what, will, the의 정보를 기억하고 있고
-> 출력층은 cat이 다음일 확률을 가장 높게 예측하도록 학습된다.

#### 각 레이어에서는 무슨 일이 일어날까?

1. 임베딩층
-> 입력 : fat의 원-핫 벡터
-> 임베딩 행렬(look-up-table) E와 곱하여 저차원 벡터로 변환된다
-> 이 벡터가 RNN의 입력이 된다.

2. 은닉층
-> 입력 벡터 xt와 이전 은닉 상태 ht-1으로 현재 은닉 상태 ht를 계산한다

3. 출력층 
-> 은닉 상태 ht를 받아서 다음 단어 확률을 계산한다
-> yt = softmax(Wy*ht + by)
[0.01, 0.03, ..., 0.86, ..., 0.002] 등으로 출력되며 가장 큰 값이 'cat'에 대응


#### 손실함수 : 크로스 엔트로피

실제 정답이 cat이라면, cat에 해당하는 인덱스가 1인 원-핫 벡터와 예측값을 비교하여 손실을 계산한다.

![alt text](img/RNN/가중치정리.png)

