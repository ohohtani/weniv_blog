# RNN 쉽게쉽게 풀어쓰기

## 순환 신경망

먼저 RNN이란?
-> Recurrent Neural Network의 줄임말, 순서를 가진 데이터(시퀀스)를 잘 처리할 수 있는 신경망.
예를 들어 '문장'은 단어들이 순서대로 나열된 것이므로 RNN이 잘 작동한다. (입력도 문장, 출력도 문장이므로)

일반적인 신경망은 정보가 한 방향(입력 -> 출력)으로만 흐르지만, RNN은 자기 자신한테도 정보를 보낸다.
즉, 어떤 시점에 계산된 결과를 다음 시점의 입력으로도 사용한다. so, 과거 정보를 기억하는 것이 가능하다.

이걸 위해 셀(cell)이라는 장치를 쓰는데, 이것은 과거 정보를 담아두는 메모리 역할을 한다.
and 셀이 다음으로 전달하는 값을 '은닉 상태'라고 부름

예를 들어 t시점의 셀은:
-> 지금 입력, 바로 전 은닉 상태(ht-1) 이 두가지를 가지고 계산하여 지금의 은닉상태 ht를 만들고 이것을 출력하거나 다음 셀에 넘긴다

![alt text](img/RNN/rnn구조조.png)

RNN은 입력과 출력의 길이를 자유롭게 정할 수 있어서 여러 작업에 활용된다.

One to Many (하나 -> 여러 개)
- ex) 이미지 한 장에서 문장 출력 (이미지 캡션 만들기)
Many to One (여러 개 -> 하나)
- ex) 문장을 보고 긍정/부정 판단하기
Many to Many (여러 개 -> 여러 개)
- ex) 문장을 다른 언어로 번역, 챗봇, 개체명 인식 등

[수식 이해하기]
![alt text](img/RNN/수식이해.png)

--- 대충 나와있어서 다시할게요 ---

RNN 입력/출력 차원
- 입력 x의 차원 : input_dim
- 은닉 상태 ht의 차원 : hidden_units
- 출력 y의 차원 : 문제에 따라 다름 (예: 이진 분류면 1)

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN

model = Sequential()
model.add(SimpleRNN(3, input_shape=(2, 10)))
```

3은 은닉 상태의 크기이며, 2는 시점의 수(timesteps) 
10은 입력 벡터의 크기이다.





















