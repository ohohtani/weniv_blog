# RNN 쉽게쉽게 풀어쓰기

## 순환 신경망

먼저 RNN이란?
-> Recurrent Neural Network의 줄임말, 순서를 가진 데이터(시퀀스)를 잘 처리할 수 있는 신경망.
예를 들어 '문장'은 단어들이 순서대로 나열된 것이므로 RNN이 잘 작동한다. (입력도 문장, 출력도 문장이므로)

일반적인 신경망은 정보가 한 방향(입력 -> 출력)으로만 흐르지만, RNN은 자기 자신한테도 정보를 보낸다.
즉, 어떤 시점에 계산된 결과를 다음 시점의 입력으로도 사용한다. so, 과거 정보를 기억하는 것이 가능하다.

이걸 위해 셀(cell)이라는 장치를 쓰는데, 이것은 과거 정보를 담아두는 메모리 역할을 한다.
and 셀이 다음으로 전달하는 값을 '은닉 상태'라고 부름

예를 들어 t시점의 셀은:
-> 지금 입력, 바로 전 은닉 상태(ht-1) 이 두가지를 가지고 계산하여 지금의 은닉상태 ht를 만들고 이것을 출력하거나 다음 셀에 넘긴다

![alt text](img/RNN/rnn구조조.png)

RNN은 입력과 출력의 길이를 자유롭게 정할 수 있어서 여러 작업에 활용된다.

One to Many (하나 -> 여러 개)
- ex) 이미지 한 장에서 문장 출력 (이미지 캡션 만들기)
Many to One (여러 개 -> 하나)
- ex) 문장을 보고 긍정/부정 판단하기
Many to Many (여러 개 -> 여러 개)
- ex) 문장을 다른 언어로 번역, 챗봇, 개체명 인식 등

[수식 이해하기]
![alt text](img/RNN/수식이해.png)

--- 대충 나와있어서 다시할게요 ---

RNN 입력/출력 차원
- 입력 x의 차원 : input_dim
- 은닉 상태 ht의 차원 : hidden_units
- 출력 y의 차원 : 문제에 따라 다름 (예: 이진 분류면 1)

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN

model = Sequential()
model.add(SimpleRNN(3, input_shape=(2, 10)))
```

3은 은닉 상태의 크기이며, 2는 시점의 수(timesteps) 
10은 입력 벡터의 크기이다.

#### RNN 직접 구현해보기
```python
import numpy as np

inputs = np.random.random((10,4))   # 시점 10개, 입력 차원 4
hidden_state = np.zeros((8,))       # 은닉 상태 크기 8로 초기화

Wx = np.random.random((8,4))        # 입력 가중치
Wh = np.random.random((8,8))        # 은닉 상태 가중치
b = np.random.random((8,))          # 편향

for x in inputs:
    output = np.tanh(np.dot(Wx, x) + np.dot(Wh, hidden_state) + b)
    hidden_state = output
```
매 시점마다 입력과 은닉 상태를 가지고 현재의 은닉 상태를 구한다
결과는 과거 정보를 반영한 벡터가 된다


#### 양방향 RNN
```python
from tensorflow.keras.layers import Bidirectional

model = Sequential()
model.add(Bidirectional(SimpleRNN(128, return_sequences=True), input_shape=(10,5)))
```
앞 + 뒤 방향 정보를 모두 사용하여 더 정확한 예측을 한다 ex) 빈칸 채우기


## LSTM에 대하여

### 바닐라 RNN의 한계

바닐라 RNN : 가장 기본적인 RNN
RNN은 앞 시점의 정보를 기억하면서 다음 시점에 영향을 주지만, 뒤로 갈수록 정보가 점점 희미해진다. (장기 의존성 문제)

![alt text](img/RNN/바닐라.png)

바닐라 RNN은 아주 단순하게 위와 같이 작동한다
- 현재 입력 xt + 이전 은닉 상태 ht-1 -> tanh 함수 통과 -> 현재 은닉 상태 ht

이 은닉 상태 ht는 출력으로도 쓰이고, 다음 시점의 입력으로도 넘어간다.
하지만 이 구조에서는 tanh 같은 함수를 지나면서 정보가 작아지고 흐릿해지기 때문에 기억력이 약해진다.

### LSTM : 기억력 좋은 RNN

핵심 아이디어는 기억해야할 건 기억하고, 잊을 건 잊자! 마인드 
RNN에 3개의 게이트를 추가하여 지울 것, 저장할 것, 내보낼 것을 정해서 정보를 똑똑하게 다룬다.

![alt text](img/RNN/lstm.png)

구성 요소 
- 셀 상태(ct) : 진짜 장기 기억, 과거 정보를 직접 전달하는 선
- 은닉 상태(ht) : 단기 출력, 다음 셀이나 출력층으로 가는 값
- 입력 게이트 : 지금 정보 중 뭘 기억할까?
- 삭제 게이트 : 이전 정보 중 뭘 지울까?
- 출력 게이트 : 뭘 꺼내서 출력할까?

### 게이트 하나씩 보기

![alt text](img/RNN/입력게이트.png)

입력게이트는 지금 들어온 정보 중, 뭘 기억해야할지 정한다.

현재 입력 xt + 이전 은닉 상태 ht-1 에 각각 가중치를 곱하고 편향을 더한 뒤, sigmoid를 씌워 얼마나 기억할지 결정한다 (0~1)
sigmoid 대신 tanh 를 씌우면 -1~1 의 범위가 된다.

![alt text](img/RNN/삭제게이트.png)

삭제게이트는 예전 기억 중에서, 무엇을 버려야할지 정한다

현재 입력 xt와 이전 은닉 상태 ht-1를 가중치 곱하고 시그모이드를 씌워 0~1의 값을 추출한다.
0에 가까울수록 정보가 많이 삭제된 것이고, 1에 가까울수록 정보를 온전히 기억한 것이다. 

![alt text](img/RNN/셀상태.png)

셀 상태는 실제로 기억을 유지하는 공간을 의미한다.

삭제게이트 ft와 이전 시점의 셀 상태를 곱하여 예전 기억의 일부를 삭제한다
(ft는 0~1 사이의 숫자 벡터이고 ct-1은 이전 시점의 셀 상태이므로 이전 시점의 셀 상태를 완전히 유지하거나 일부만 기억하는 것)

그리고 it와 gt의 원소곱한 결과를 더해주는데,
LSTM의 셀 상태 ct는 '과거 기억 중 유지할 부분'과 '지금 새로 저장할 부분'을 더한 것이므로
예전기억의 일부(ft*ct-1)와 지금 선택한 정보(itㅇgt) 를 더하여 셀 상태를 업데이트한다.

입력 게이트 it는 어떤 정보를 얼마나 기억할지를 조절하는 벡터이고,
기억 후보 gt는 기억하고 싶은 정보 자체이기 때문에,
이 둘을 '같은 위치끼리 곱'해서 '부분적으로 기억'하는 효과를 만드는 것이다.

![alt text](img/RNN/출력게이트.png)

출력 게이트는 현재 시점의 은닉 상태 ht를 만들기 위한 게이트이다.
즉, 셀 상태 ct에서 나온 정보를 바로 출력하지 않고, 필터를 한 번 더 거치는 것
그렇게 시그모이드를 거쳐 나온 값과 셀 상태(탄젠트를 거친)를 원소곱하여 은닉 상태 ht를 만든다

셀 상태를 탄젠트랑 왜 곱할까? 
-> 셀 상태 ct는 정보가 쌓여 있어서 값이 매우 클 수 있다. 그래서 직접 출력하지 않고 출력하기 전에 tanh를 씌워 -1~1 범위로 압축한다.

원소곱을 왜 할까?
-> 출력 게이트 ot는 '지금 셀 상태에서 어떤 정보를 얼마만큼 꺼낼지'를 위치별로 세밀하게 조절하기 위해 원소곱을 한다.

왜 덧셈과 행렬곱은 안되는가?
-> 원소곱을 하면 각 값 하나하나에 대한 별도 제어가 가능하다 (샤워기 구멍마다 벨브가 따로 있음, 어떤 구멍은 물을 다 내보내고, 어떤 건 잠그고, 어떤 건 절반만 가능)
-> 덧셈을 하면 게이트가 출력 양을 조절하는 역할을 못한다 (그냥 물의 세기만 높아짐, 어디로 나오는지는 조절 못함)
-> 행렬곱은 위치별로 꺼낼지 말지를 제어할 수는 없다 (구멍 위치를 바꾸거나, 수압을 섞을 순 있으나 출력 위치 자체만 바꿀 뿐, 꺼낼지 말지에 대한 판단을 못함)